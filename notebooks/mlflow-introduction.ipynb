{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d546b38-f8b7-438d-9808-abfa3c61c993",
   "metadata": {},
   "source": [
    "# Tutorial : introduction to MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d0f60f-e70f-41f4-99ed-1e9800cfa9f7",
   "metadata": {},
   "source": [
    "This first application introduces the basic concepts of `MLFlow`. The goal is to predict the income class of individuals using a sample data from the US Census and a Random Forest classifier using the popular [scikit-learn](https://scikit-learn.org/stable/) machine learning `Python` library. \n",
    "\n",
    "We first illustrate how we would perform the training and fine-tuning of the model in a traditional way. Then, we show how we can integrate it as an **MLflow experiment**, so as to **log** relevant parameters and metrics in `MLflow`'s **tracking server** and visualize them in the UI. Finally, we illustrate how selected models can transition from the tracking server to the **model registry**, and how they can then be used from there to perform inference on new data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ed5cf-810d-47ea-ade3-f92f5e3771be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.pyfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba85244-30af-43f4-92f2-a15bf67cf16b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b67a65-102f-4069-8cc8-4c64dcdc00ec",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed086e6",
   "metadata": {},
   "source": [
    "For this application, we'll use a classical dataset extracted from the 1994 US Census bureau data. The goal is to determine whether a person makes over $50K a year ('>50K') or less ('<=50K') using sociodemographic characteristics on the selected individuals. As the available variables are generally self-explanatory, we won't describe the data much, but more information on them can be found in the original [Kaggle challenge](https://www.kaggle.com/datasets/uciml/adult-census-income)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ae610-bb74-405b-9df0-06d423fd03c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_URL = \"https://minio.lab.sspcloud.fr/projet-formation/diffusion/mlops/data/adult-census-us.csv\"\n",
    "df_census = pd.read_csv(DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322565d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_census.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1291ee38",
   "metadata": {},
   "source": [
    "The goal is to predict the income class, so we must first set it aside from the training data. As this variable consists in string-encoded categories, we must encode it in a numerical format to be able to feed it to a machine learning model. A common practice for ordinal data (data for which an order exists, such as income class) is to encode labels as subsequent integers starting at 0, a technique known as **label encoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30313ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "X = df_census.drop(columns=\"class\")\n",
    "y = le.fit_transform(df_census[\"class\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c770dee4",
   "metadata": {},
   "source": [
    "These new integer-encoded categories can naturally be mapped to the original values of the variable, and conversely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe46f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The encoded classes\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c732d24c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The corresponding original classes\n",
    "print(y)\n",
    "print(np.array([le.classes_[i] for i in y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f2f3e",
   "metadata": {},
   "source": [
    "A common practice in machine learning projects is to start by setting a fraction of the data aside as a **test dataset**. This data will be used at the very end of the project in order to properly evaluate the generalization performance of our selected algorithm, i.e. how it would perform on new unseen data. The rest of the data (**training dataset**) will be used to train the algorithms and compare their performance. Without this step, we are at risk of overfitting our models on the available data so that our evaluation metrics would no longer properly estimate the generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fded59-b6f4-4647-809b-643bef3b056a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c259a228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab462d9",
   "metadata": {},
   "source": [
    "These general information show that thousands of observations are missing for some variables. To avoid wasting data and since these might not be missing-at-random, we'll impute values for the missing ones :\n",
    "- for numerical variables, we'll impute the median of the variable\n",
    "- for categorical variables, we'll impute the mode, i.e. the most frequent category in the data\n",
    "\n",
    "As previously, string-encoded categorical variables must also be converted to some form of numerical data. We'll use the same encoding strategy as the one used to encode the target variable.\n",
    "\n",
    "So as to make all these steps as reproducible as possible, we formalize them as a `scikit-learn` `Pipeline` object. More information on their justification and the way there are used can be found in the [documentation](https://scikit-learn.org/stable/modules/compose.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0135c24-826e-47e3-931c-7db4ec20b9dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "median_imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "mode_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "\n",
    "categorical_transformer = make_pipeline(mode_imputer, ordinal_encoder)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numerical\", median_imputer, make_column_selector(dtype_include=np.int64)),\n",
    "        (\"categorical\", categorical_transformer, make_column_selector(dtype_include=object))\n",
    "    ], remainder=\"passthrough\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ab4ca8",
   "metadata": {},
   "source": [
    "As most `scikit-learn` objects, the pipeline must first `fit` the data (e.g. compute the most frequent value or median). Then, we can use it to `transform` the data. The resulting object is a `NumPy` array with the same structure as the original data. It is not very useful per se, but it shows us that the categorical variables are indeed transformed into numerical values. This numerical array can then be fed to a machine learning model to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58964862-1e9f-4fca-b158-4891057db7a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb81610-0b95-4dcd-96b0-ec2eebbc7281",
   "metadata": {},
   "source": [
    "## Tracking machine learning experiments : the classical way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506fd5be",
   "metadata": {},
   "source": [
    "In order to really understand why the MLOps approach is desirable, we must first get an idea of how we would train our model without it, i.e. the \"classical\" way. The workflow we are trying to achieve is best described by the following figure from the [scikit-learn documentation](https://scikit-learn.org/stable/).\n",
    "\n",
    "<img src=\"img/grid_search_workflow.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Using the training data, we want to train a model so as to get the best generalization performance, i.e. minimize the prediction error on unseen data. To do so, we have to **fine-tune** the **hyperparameters** of our model, i.e. find the combination of **hyperparameters** that provide the best performance. In order to avoid **overfitting** when doing so, we use a procedure called **cross-validation** (described in details [here](https://scikit-learn.org/stable/modules/cross_validation.html)). When we have found the optimal set of hyperparameters, we use the model trained with those for a final evaluation on the test set.\n",
    "\n",
    "In this example, we train a *Random Forest* to discriminate the two income classes. First, we build a `Pipeline` object that integrates the preprocessing step as well as the model, so as to be able to improve reproducibility of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef674c48-5111-40af-81d4-02c0ec18ad22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state=SEED)\n",
    "\n",
    "pipe_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('classifier', rf_clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39f87d5",
   "metadata": {},
   "source": [
    "Although the hyperparameters provided natively by `scikit-learn` are usually good defaults, we will of course want to check whether we can improve the performance further by **fine-tuning** the relevant hyperparameters. To do so, we use the *grid search* method, which amounts to testing all the possible hyperparameters combinations along given values (*grid*) for these hyperparameters. For each combination, a performance evaluation is performed using a 5-folds *cross-validation*. As the accuracy is rarely a relevant metric for classification problems because of class imbalance, we also request the precision, the recall and the f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819fa1df-292f-4c3d-b9af-c2e164f391a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [50, 100, 200],\n",
    "    \"classifier__max_leaf_nodes\": [5, 10, 50]\n",
    "}\n",
    "\n",
    "pipe_gscv = GridSearchCV(pipe_rf, \n",
    "                         param_grid=param_grid, \n",
    "                         scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\"],\n",
    "                         refit=\"f1\",\n",
    "                         cv=5, \n",
    "                         n_jobs=5, \n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c3f16",
   "metadata": {},
   "source": [
    "**Question** : can you guess the total number of `fit` steps that will be performed when calling the `fit` method on the `pipe_gscv` object ?\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "    <font size=\\\"3\\\" color=\\\"darkgreen\\\"><b>Click to see the answer </b></font>\n",
    "</summary>\n",
    "\n",
    "From the grid search only, there are 3 * 3 = 9 candidate models to train. However, for each combination, a 5-folds cross-validation is performed, which involves 5 training steps (*fits*). So altogether, there will be 9 * 5 = 45 *fits* to compute.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd56281e-31dc-4a24-940e-6eb180aad8f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe_gscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f217e9ab",
   "metadata": {},
   "source": [
    "We can get detailed results for each candidate model in a `Pandas DataFrame`. This enables us to compare the models and select the best candidate based on their respective performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4541f8d2-7fac-4407-928d-b70c3640fa8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gscv_results = pd.DataFrame(pipe_gscv.cv_results_)\n",
    "gscv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb75b873",
   "metadata": {},
   "source": [
    "The fitted `Pipeline` object actually keep tracks of the best model for us. We can thus show the best performing set of hyperparameters, and use the model trained with these hyperparameters to compute the final score on the test set (not used until yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23990354-0989-454f-a1d3-c2f230d82d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(pipe_gscv.best_params_)\n",
    "\n",
    "best_model = pipe_gscv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042c22b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test_pred = best_model.predict(X_test)\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Final F1-score on test data : {f1_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e06080",
   "metadata": {},
   "source": [
    "In order for this analysis to be reproducible, we must find a way to export the results. Fortunately, `scikit-learn` models are serializable. One way to persist them is to use `joblib` (see the [documentation on model persistence](https://scikit-learn.org/stable/model_persistence.html) for a more detailed discussion on possible way to export models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca46c05-9f19-4269-9311-ccb547d40ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"models/\"):\n",
    "    os.makedirs(\"models/\")\n",
    "joblib.dump(pipe_gscv, 'models/pipeline_train_model_20230118.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f985c76",
   "metadata": {},
   "source": [
    "This is convenient for the development phase, but it is also very clear that **this way of persisting models is not production-grade nor scalable** :\n",
    "- first and foremost, we lack a proper way to **track experiments** (data used for training, environment configuration, metrics...)\n",
    "- we can not easily visualize the various metrics so as to compare and select the best model\n",
    "- we can parallelize the cross-validation computation, but we can't readily parallelize the evaluation of each hyperparameters combination\n",
    "- there is no easy and standardized way to distribute the serialized models, so the collaboration of several team members on a given experiment is complicated\n",
    "\n",
    "The MLOps principles were precisely devised to solve these various problems. Let's see now how MLflow enables us to implement them easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbc5b57-6dcc-4bf1-a2c3-29ae13300e46",
   "metadata": {},
   "source": [
    "## Tracking machine learning experiments : the MLFlow way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259b194e-cbdf-463a-a7fd-e71ce0ddae19",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c359c4",
   "metadata": {},
   "source": [
    "The main component of `MLflow` is the *Tracking Server*, which tracks experiments and save the relevant data and metadata. More precisely, for each *run* (\"execution of some piece of data science code\"), the *Tracking Server* records : \n",
    "- **experiments data** (parameters, metrics, tags, notes, metadata, ...) in a **backend store** (in our case, a `PostgreSQL` database)\n",
    "- **artifacts** (models, files, images, ...) in an **artifact store** (in our case, `S3`-like storage)\n",
    "\n",
    "As a user, we communicate with the *Tracking Server* through a client (in our case, a `Jupyter` notebook with a `Python` kernel).\n",
    "\n",
    "Fortunately, in a properly set up environment, these three communication levels can be pre-configured so that they are relatively transparent to the user. This enables the data scientist to focus on the business task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c0dc8f",
   "metadata": {},
   "source": [
    "<img src=\"img/mlflow-tracking.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a155b49",
   "metadata": {},
   "source": [
    "The client must know the URI of the *tracking server*. If a `MLflow` instance has been launched on the SSP Cloud previous to the client, the client will automatically discover the URI. If not, it must be set manually. Opening this URL opens the UI of `MLflow`, which we will be using later in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a2e959",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatic discovery : if MLFlow has been launched before Jupyter/VSCode\n",
    "if \"MLFLOW_TRACKING_URI\" in os.environ:\n",
    "    print(os.environ[\"MLFLOW_TRACKING_URI\"])\n",
    "else:\n",
    "    print(\"MLflow was not automatically discovered, a tracking URI must be provided manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15061562-8b91-45c4-8ada-f8458873e513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Manual configuration : if MLFlow has been launched after Jupyter/VSCode\n",
    "# os.environ[\"MLFLOW_TRACKING_URI\"] = \"copy_uri_from_mlflow_service_README_here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b55dd7-1f2c-42fa-941d-bad8412cdd16",
   "metadata": {},
   "source": [
    "### Tracking experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa949cb-bf32-4ce7-9bfe-7b7dc3ec00df",
   "metadata": {},
   "source": [
    "In the previous steps, we fine-tuned our model, i.e. we trained the same model with several different combinations of hyper-parameters in order to ultimately select the one with the best performance according to a given metric. In comparison with the \"traditional way\" we saw above, `MLflow` enables us to track these experiments in a much more refined way, compatible with the *MLOps* principles.\n",
    "\n",
    "The function `log_gsvc_to_mlflow` below enables us to convert the data contained in our `GridSearchCV` object (hyperparameters, metrics, artifact..) into an `MLflow` experiment, which can then be queried using the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd9585f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_gsvc_to_mlflow(gscv, mlflow_experiment_name):\n",
    "    \"\"\"Log a scikit-learn trained GridSearchCV object as an MLflow experiment.\"\"\"\n",
    "     # Set up MLFlow context\n",
    "    mlflow.set_experiment(experiment_name=mlflow_experiment_name)\n",
    "\n",
    "    for run_idx in range(len(gscv.cv_results_[\"params\"])):\n",
    "        # For each hyperparameter combination we trained the model with, we log a run in MLflow\n",
    "        run_name = f\"run {run_idx}\"\n",
    "        with mlflow.start_run(run_name=run_name):\n",
    "            # Log hyperparameters\n",
    "            params = gscv.cv_results_[\"params\"][run_idx]\n",
    "            for param in params:\n",
    "                mlflow.log_param(param, params[param])\n",
    "\n",
    "            # Log fit metrics\n",
    "            scores = [score for score in gscv.cv_results_ if \"mean_test\" in score or \"std_test\" in score]\n",
    "            for score in scores:\n",
    "                mlflow.log_metric(score, gscv.cv_results_[score][run_idx])\n",
    "\n",
    "            # Log model as an artifact\n",
    "            mlflow.sklearn.log_model(gscv, \"gscv_model\")\n",
    "\n",
    "            # Log training data URL\n",
    "            mlflow.log_param(\"data_url\", DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f397a-1d6d-47de-94ca-29a2957c3427",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_gsvc_to_mlflow(gscv=pipe_gscv, mlflow_experiment_name=\"tutorial-mlflow-intro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7228d7-d1ca-4188-9a7d-c3effff3e693",
   "metadata": {},
   "source": [
    "### Application 1.1: Tracking models with MLflow\n",
    "\n",
    "If the previous cell executed correctly, that means the experiments and in particular all the data we wanted `MLflow` to log are now available in the *tracking server*. In order to interact with these data and try to select the best model, we'll learn to use the UI. Please follow the following steps :\n",
    "1. Open the UI using the URI we printed above\n",
    "2. In the *Experiments* tab, open the \"tutorial-mlflow-intro\" experiment\n",
    "3. Verify that there are indeed 9 runs that have been recorded, one for each hyperparameters combination\n",
    "4. Open a given run and verify that you can retrieve the various information we wanted to log (hyperparameters, evaluation metrics, training data URL), check that you can download the serialized `scikit-learn` model, and check the `requirements.txt` file to understand how `MLflow` automatically inferred the required `Python` environment\n",
    "5. Go back to the list of runs by clicking again on the \"tutorial-mlflow-intro\" experiment\n",
    "6. Add additional columns using the *Columns* drop-down menu in the *Table* panel\n",
    "7. Sort the models in descending order according to the **mean test F1-score** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b48f501-3446-49af-b75b-b53dbd84fc3b",
   "metadata": {},
   "source": [
    "### Application 1.2: Registering a model with MLflow\n",
    "In the previous section, we logged properly our experiment in the `MLflow` tracking server, which enables to compare our models and see the best performing ones in a visual way. Now, we want to be able to select a model, put it in production, and allow the other members of the projet to query it. For this to be possible, we have to **move the model from the tracking server to the model registry**.\n",
    "1. Consider both the high mean test F1-score and low standard deviation of test F1-score to make a decision on the best model.\n",
    "2. Click on the run corresponding to your chosen best model.\n",
    "3. Click on \"Register Model\"\n",
    "4. Create a New Model and give it a relevant name (e.g. \"rf-census\")\n",
    "5. Move to the model registry by clicking on the \"Models\" tab\n",
    "6. If everything worked correctly, you should see your model in the list of the registered models. Click on it to get the list of the registered versions of the model. For now, there is only one version as we pushed it only one time.\n",
    "7. Click on \"Version 1\" to get the information on this specific version of the model. Two things are especially interesting to note :\n",
    "\n",
    "   **a.** The \"Stage\" section. Here you can indicate to all members of the project what is stage of this specific model. Let's transition it to \"Production\" to indicate that is our reference model, which we want to deploy\n",
    "\n",
    "   **b.** The \"Source run\" section. Here you can retrieve the run that corresponds to this model. If you click on the run id, you retrieve all the information we logged (environment, metrics, artifacts location...). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de39735-cd52-46bb-9559-81c6f393b3c5",
   "metadata": {},
   "source": [
    "### Querying a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d910ff9-4b0c-46f0-8d5f-47c5a6b49a79",
   "metadata": {},
   "source": [
    "As above, let's perform the final evaluation on the test set using the model we put in production. We can retrieve the model either by its version or by its stage, should it have one. We fetch the model using the `mlflow.pyfunc.load_model` function. We then have our `scikit-learn` model, which can directly be used for prediction. Let's check that we find the same final F1-score on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b4d454-6e9d-4a18-a144-ec534a647dc6",
   "metadata": {},
   "source": [
    "#### Using the version number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7520d8-8e7c-418e-acb6-5daac6ecc2ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch the model\n",
    "model_name = \"rf-census\"\n",
    "version = 1\n",
    "\n",
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecfcbe7-2a23-47ea-8b75-d910fbae51ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "y_test_pred = model.predict(X_test)\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Final F1-score on test data : {f1_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7391615-3290-47b8-a3b2-f7c9662774f9",
   "metadata": {},
   "source": [
    "This score indeed corresponds to the one we found in the \"classical ML training\" section using the best trained model !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb1adeb-04ae-43c7-90a1-e09212e8063b",
   "metadata": {},
   "source": [
    "#### Using the stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb635aa-2a6f-4acb-b411-c8d71af2878d",
   "metadata": {},
   "source": [
    "Equivalently, we can use the stage of the model, which should produce the same score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a266fc1a-b560-4125-a9c8-0dee5c71ae98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch the model\n",
    "model_name = \"rf-census\"\n",
    "stage = 'Production'\n",
    "\n",
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{stage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7615eb88-187a-4af8-8d5f-86a907462f63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "y_test_pred = model.predict(X_test)\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Final F1-score on test data : {f1_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b0c365-d352-4748-a5dc-44cb144d490a",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e1325-5566-4816-95ac-d0c6c69791cf",
   "metadata": {},
   "source": [
    "`MLflow` enables us to **set our machine learning experiments to the standards of the `MLOps` approach** in a very user-friendly way :\n",
    "- data scientists can very easily decide what data they want to log for each experiment so as to **keep a detailed track of those experiments**\n",
    "- other members of the team (e.g. data engineers that might be in charge of deploying the model) can very easily fetch the model and use it for prediction in an application. To do so, they only need to know the name of the model as well as its version or its stage, but not the actual location of the artifact on the storage, as this layer is abstracted by `MLflow`. **This makes collaboration on machine learning projects very convenient and efficient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aaa7d9-4ae6-42da-8de7-de6026b73e56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3fa046f995eb80ac40c0869a1f9df46519f4ada8b8c395ef25dd1aa1a1a2fc63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
