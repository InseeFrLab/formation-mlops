{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: An introduction to MLOps with MLflow\n",
        "subtitle: |\n",
        "  [**[Romain Avouac (Insee), Thomas Faria (Insee), Tom Seimandi (Insee)]{.orange}**]{.orange}\n",
        "# date: \n",
        "slide-number: true\n",
        "footer: |\n",
        "  An introduction to MLOps with MLflow\n",
        "# uncomment for French presentations:\n",
        "# lang: fr-FR\n",
        "# for blind readers:\n",
        "slide-tone: false\n",
        "# for @olevitt:\n",
        "chalkboard: # press the B key to toggle chalkboard\n",
        "  theme: whiteboard\n",
        "# uncomment to use the multiplex mode:\n",
        "#multiplex: true\n",
        "format:\n",
        "  # pick the light mode (onyxia-revealjs) or the dark mode (onyxia-dark-revealjs)\n",
        "  onyxia-revealjs:\n",
        "  #onyxia-dark-revealjs:\n",
        "    incremental: true \n",
        "    output-file: index.html\n",
        "controls: true\n",
        "css: custom.css\n",
        "from: markdown+emoji\n",
        "---"
      ],
      "id": "b4024a50"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "## Who are we ?\n",
        "\n",
        "- [**Data scientists**]{.orange} at Insee\n",
        "    - methodological and IT innovation teams\n",
        "    - support data science projects\n",
        "\n",
        "- [**Contact us**]{.orange}\n",
        "  - <romain.avouac@insee.fr>\n",
        "  - <thomas.faria@insee.fr>\n",
        "  - <tom.seimandi@insee.fr>\n",
        "\n",
        "## Context\n",
        "\n",
        "- Difficulty of transitioning from experiments to [**production-grade**]{.orange} machine learning systems\n",
        "\n",
        "- Leverage [**best practices**]{.orange} from software engineering\n",
        "  - Improve [**reproducibility**]{.blue2} of analysis\n",
        "  - [**Deploy**]{.blue2} applications in a [**scalable**]{.blue2} way\n",
        "\n",
        "## The DevOps approach\n",
        "\n",
        "- [**Unify**]{.orange} development (*dev*) and system administration (*ops*)\n",
        "  - [**shorten**]{.blue2} development time\n",
        "  - maintain software [**quality**]{.blue2} \n",
        "\n",
        ". . .\n",
        "\n",
        "![](img/devops.png){fig-align=\"center\" height=300}\n",
        "\n",
        "## The MLOps approach\n",
        "\n",
        "- Integrate the [**specificities**]{.orange} of machine learning projects\n",
        "  - [**Experimentation**]{.blue2}\n",
        "  - [**Continuous improvement**]{.blue2}\n",
        "\n",
        ". . .\n",
        "\n",
        "![](img/mlops.png){fig-align=\"center\" height=400}\n",
        "\n",
        "## MLOps : principles\n",
        "\n",
        "- [**Reproducibility**]{.orange}\n",
        "\n",
        "- [**Versioning**]{.orange}\n",
        "\n",
        "- [**Automation**]{.orange}\n",
        "\n",
        "- [**Monitoring**]{.orange}\n",
        "\n",
        "- [**Collaboration**]{.orange}\n",
        "\n",
        "## Why MLflow ?\n",
        "\n",
        "- Multiple [**frameworks**]{.orange} implement the MLOps principles\n",
        "\n",
        "- Pros of `MLflow`\n",
        "  - [**Open-source**]{.blue2}\n",
        "  - Covers the whole [**ML lifecycle**]{.blue2}\n",
        "  - [**Agnostic**]{.blue2} to the ML library used\n",
        "  - We have [**experience**]{.blue2} with it\n",
        "\n",
        "## Training platform : the SSP Cloud\n",
        "\n",
        "- An [**open innovation production-like**]{.orange} environment\n",
        "  - [**Kubernetes**]{.blue2} cluster\n",
        "  - S3-compatible [**object storage**]{.blue2}\n",
        "  - Large computational [**resources**]{.blue2} (including GPUs)\n",
        "\n",
        "- Based on the [Onyxia](https://github.com/InseeFrLab/onyxia-web) project\n",
        "  - User-friendly [interface](https://datalab.sspcloud.fr/) to launch data science services\n",
        "  - A [catalog of services](https://datalab.sspcloud.fr/catalog/ide) which covers the full lifecycle of data science projects\n",
        "\n",
        "## Outline\n",
        "\n",
        ":one: Introduction to MLFlow\n",
        "\n",
        ". . .\n",
        "\n",
        ":two: Deploying a model as an API\n",
        "\n",
        ". . .\n",
        "\n",
        ":three: Distributing the hyperparameter optimization\n",
        "\n",
        "\n",
        "\n",
        "## Application 0\n",
        "\n",
        ":::{.callout-tip collapse=\"true\" icon=false}\n",
        "## Preparation of the working environment\n",
        "\n",
        ":::{.incremental}\n",
        "1. Create an account on the [SSP Cloud](https://datalab.sspcloud.fr/home) using your professional mail address\n",
        "2. Launch a `MLflow` service by clicking [this URL](https://datalab.sspcloud.fr/launcher/automation/mlflow?autoLaunch=true)\n",
        "3. Launch a `VSCode` service by clicking [this URL](https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=true&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2FInseeFrLab%2Fformation-mlops%2Fmain%2Finit.sh»)\n",
        "4. Open the `VSCode` service and input the service password (either automatically copied or available in the `README` of the service)\n",
        "5. You're all set !\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Introduction to MLFlow\n",
        "\n",
        "## Tracking server\n",
        "\n",
        "- \"An [**API**]{.orange} and [**UI**]{.orange} for [**logging**]{.orange} parameters, code versions, metrics, and artifacts\"\n",
        "\n",
        ". . .\n",
        "\n",
        "![](img/mlflow-tracking.png){fig-align=\"center\" height=400}\n",
        "\n",
        "## Projects\n",
        "\n",
        "- \"A standard format for [**packaging**]{.orange} reusable data science code\"\n",
        "\n",
        ". . .\n",
        "\n",
        "![](img/mlflow-projects.png){fig-align=\"center\" height=400}\n",
        "\n",
        "## Models\n",
        "\n",
        "- \"A convention for [**packaging**]{.orange} machine learning [**models**]{.orange} in multiple [**flavors**]{.orange}\"\n",
        "\n",
        ". . .\n",
        "\n",
        "![](img/mlflow-models.png){fig-align=\"center\" height=400}\n",
        "\n",
        "## Model registry\n",
        "\n",
        "- \"A [**centralized model store**]{.orange}, set of APIs, and UI, to [**collaboratively**]{.orange} manage the full lifecycle of an MLflow Model\"\n",
        "\n",
        ". . .\n",
        "\n",
        "![](img/mlflow-model-registry.png){fig-align=\"center\" height=400}\n",
        "\n",
        "\n",
        "\n",
        "## Application 1\n",
        "\n",
        ":::{.callout-tip collapse=\"true\" icon=false}\n",
        "## Introduction to MLflow concepts\n",
        "\n",
        ":::{.incremental}\n",
        "1. In `VSCode`, open the notebook `mlflow-introduction.ipynb` (from the `notebooks` directory)\n",
        "2. Choose our custom `Python` kernel : \n",
        "    + `Select Kernel -> Python environments... -> base (Python 3.x.x)`\n",
        "3. Execute the notebook cell by cell. Try to understand carefully how the `Python` session interacts with the `MLflow` API. Explore the `MLflow` UI and try to build your own experiments from the example code provided in the notebook.\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# A Practical Example: NACE Code Prediction for French Companies\n",
        "\n",
        "## Context\n",
        "\n",
        "- [**NACE**]{.orange}\n",
        "  - European standard classification of productive [**economic activities**]{.blue2}\n",
        "  - [**Hierarchical structure**]{.blue2} with 4 levels and 615 codes\n",
        "\n",
        "- At Insee previously handled by an outdated [**rule-based**]{.orange} algorithm\n",
        "\n",
        "- [**Common problematic**]{.orange} to all National statistical institutes\n",
        "\n",
        "## Data used {.scrollable}\n",
        "\n",
        "::: {.panel-tabset}\n",
        "\n",
        "### Slide \n",
        "\n",
        "- A simple use-case with only [**2 variables**]{.orange}:\n",
        "  - [**Textual description**]{.blue2} of the activity – [text]{.green2}\n",
        "  - [**True NACE code**]{.blue2} labelised by the rule-based engine – [nace]{.green2} (732 modalities)\n",
        "\n",
        "- Standard [**preprocessing**]{.orange}:\n",
        "  - lowercasing\n",
        "  - punctuation removal\n",
        "  - number removal\n",
        "  - stopwords removal\n",
        "  - stemming\n",
        "  - ...\n",
        "\n",
        "\n",
        "### Raw\n",
        "\n",
        "```{ojs}\n",
        "viewof table_data = Inputs.table(transpose(data_raw), {\n",
        "    rows: 22\n",
        "})\n",
        "```\n",
        "\n",
        "### Preprocessed\n",
        "\n",
        "```{ojs}\n",
        "viewof table_data_prepro = Inputs.table(transpose(data_prepro), {\n",
        "    rows: 22\n",
        "})\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "## MLflow with a non standard framework\n",
        "\n",
        "::: {.nonincremental}\n",
        "\n",
        ":::: {.fragment fragment-index=1}\n",
        "- [**Easy to use**]{.orange} with a variety of machine learning frameworks (scikit-learn, Keras, Pytorch...) \n",
        "::::\n",
        "\n",
        ":::: {.fragment fragment-index=2}\n",
        "```python\n",
        "mlflow.sklearn.log_model(pipe_rf, \"model\")\n",
        "\n",
        "mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{version}\")\n",
        "y_train_pred = model.predict(X_train)\n",
        "\n",
        "```\n",
        "::::\n",
        "\n",
        ":::: {.fragment fragment-index=3}\n",
        "- What if we require greater [**flexibility**]{.orange} or our [**own framework**]{.orange}?\n",
        "::::\n",
        "\n",
        ":::: {.fragment fragment-index=4}\n",
        "- Possibility to [**track**]{.orange} , [**register**]{.orange} and [**deliver**]{.orange} your own model\n",
        "::::\n",
        "\n",
        ":::\n",
        "\n",
        "## MLflow with a non standard framework \n",
        "\n",
        "::: {.nonincremental}\n",
        "\n",
        ":::: {.fragment fragment-index=1}\n",
        "- There are [**2 main differences**]{.orange} when using your own framework:\n",
        "  - [**logging**]{.blue2} of parameters, metrics and artifacts\n",
        "  - [**wrapping**]{.blue2} of your custom model so that MLflow can serve it\n",
        "::::\n",
        "\n",
        ":::: {.fragment fragment-index=2}\n",
        "```python\n",
        "# Define a custom model\n",
        "class MyModel(mlflow.pyfunc.PythonModel):\n",
        "\n",
        "    def load_context(self, context):\n",
        "      self.my_model.load_model(context.artifacts[\"my_model\"])\n",
        "\n",
        "    def predict(self, context, model_input):\n",
        "        return self.my_model.predict(model_input)\n",
        "```\n",
        "::::\n",
        "\n",
        ":::\n",
        "\n",
        "<!-- By creating a class that inherits from mlflow.pyfunc.PythonModel, you are essentially creating a wrapper around your custom model that allows it to be used with the MLflow platform. The mlflow.pyfunc.PythonModel class provides a standardized interface that makes it easy to integrate your custom model with the rest of the MLflow platform. -->\n",
        "\n",
        "## Necessity of avoiding notebooks for production deployment\n",
        "\n",
        "- Arguments against using notebooks for [**ML models deployment**]{.orange}:\n",
        "  - Limited scalability for [**automation**]{.blue2} of ML pipelines.\n",
        "  - Lack of clear and [**reproducible**]{.blue2} workflows.\n",
        "  - Hinders [**collaboration**]{.blue2} and [**versioning**]{.blue2} among team members.\n",
        "  - Insufficient [**modularity**]{.blue2} for managing complex ML components.\n"
      ],
      "id": "1527cb64"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cache": false
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"../src/\")\n",
        "\n",
        "import pandas as pd\n",
        "import s3fs\n",
        "import pyarrow.parquet as pq\n",
        "from constants import TEXT_FEATURE, DATA_PATH\n",
        "from preprocessor import Preprocessor\n",
        "\n",
        "preprocessor = Preprocessor()\n",
        "fs = s3fs.S3FileSystem(\n",
        "    client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"}\n",
        ")\n",
        "df = pq.ParquetDataset(DATA_PATH, filesystem=fs).read_pandas().to_pandas()\n",
        "df = df.sample(frac=0.001, random_state=0)\n",
        "\n",
        "df_prepro = preprocessor.clean_text(df, TEXT_FEATURE)\n",
        "\n",
        "ojs_define(data_raw = df, data_prepro = df_prepro)"
      ],
      "id": "286730e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Application 2 {.scrollable}\n",
        "\n",
        ":::{.nonincremental}\n",
        ":::: {.callout-tip collapse=\"true\" icon=false}\n",
        "## Part 1 : From notebook to python scripts\n",
        "\n",
        "\n",
        "1. All scripts related to our custom model are stored in the `src` folder. Check them out. Have a look at the `MLproject` file as well.\n",
        "2. Run a training of the model using MLflow. To do so:\n",
        "\n",
        "    ```sh\n",
        "    mlflow run ~/work/formation-mlops/ --env-manager=local \\\n",
        "    -P remote_server_uri=$MLFLOW_TRACKING_URI \\\n",
        "    -P experiment_name=\"fasttext\"\n",
        "    ```\n",
        "\n",
        "3. Look at the results of your previous run:\n",
        "    + `Experiments -> fasttext -> <run_name>`\n",
        "4. You have trained the model with some default parameters. In `MLproject` check the parameters available. Re-train a model with different parameters (i.e. `dim = 25`).\n",
        "\n",
        "<details>\n",
        "<summary>\n",
        "    <font size=\\\"3\\\" color=\\\"darkgreen\\\"><b>Click to see the command </b></font>\n",
        "</summary>\n",
        "\n",
        "```sh\n",
        "mlflow run ~/work/formation-mlops/ --env-manager=local \\\n",
        "-P remote_server_uri=$MLFLOW_TRACKING_URI \\\n",
        "-P experiment_name=\"fasttext\"\n",
        "-P dim=25\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "5. In MLflow, compare the 2 models by plotting the accuracy against one parameter you have changed (i.e. `dim`)\n",
        "    + `Select the 2 runs -> Compare -> Scatter Plot -> Select your X and Y axis`\n",
        "::::\n",
        ":::\n",
        "\n",
        "\n",
        "## Application 2 {.scrollable}\n",
        "\n",
        ":::{.nonincremental}\n",
        ":::: {.callout-tip collapse=\"true\" icon=false}\n",
        "## Part 2 : Model delivery with onboarded preprocessing\n",
        "\n",
        "1. Explore the `src/train.py` file carefully. What are the main differences compare to application 1 ?\n",
        "2. Why can we say that the MLflow model onboard the preprocessing ?\n",
        "3. In MLflow, register your last model\n",
        "4. Open and run the notebook `mlflow-custom-model.ipynb` which loads your model from the model store.\n",
        "5. Read the documentation of the `predict()` function of the custom class (`src/fasttext_wrapper.py`)\n",
        "6. Make a prediction of the model\n",
        "\n",
        "<details>\n",
        "<summary>\n",
        "    <font size=\\\"3\\\" color=\\\"darkgreen\\\"><b>Click to see the command </b></font>\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "list_libs = [\"vendeur d'huitres\", \"boulanger\"]\n",
        "\n",
        "test_data = {\n",
        "    \"query\": list_libs,\n",
        "    \"k\": 1\n",
        "}\n",
        "model.predict(test_data)\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "4. Make sure that the two following descriptions give the same results: `\"COIFFEUR\"` and `\"coiffeur, & 98789\"`\n",
        "5. Change the value of the parameter `k`\n",
        "::::\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Deploying a model as an API\n",
        "\n",
        "- On détaille pas la construction d'API, on renvoie vers doc de FastAPI\n",
        "- Dire qu'on veut run l'API sous une image docker pour avoir tout ce qu'il nous faut dedans\n",
        "- Un topo sur deploiement dans kube et les fichiers deployment, service, ingress. Juste dire rapidement a quoi ils servent\n",
        "- Et truc sur Argo cd pour parler du CD\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Application 3 {.scrollable}\n",
        "\n",
        ":::{.nonincremental}\n",
        ":::: {.callout-tip collapse=\"true\" icon=false}\n",
        "## Deploying a machine-learning model as an API\n",
        "\n",
        "1. We construct a very simplistic Rest API using FastAPI. All underlying files are in the `app` folder. Check it.\n",
        "2. To deploy an API, you need to run it on a machine that contains all necessary packages, scripts, and configurations. The most convenient way is to build a Docker image containing all these specificities. Open the `Dockerfile` to see how the image is built. The image is publish via Github Actions, if interested have a look to `.github/workflows/build_image.yml`.\n",
        "3. Open the file `kubernetes/deployment.yml` and modify the highlighted lines accordingly:\n",
        "\n",
        "```{yml code-line-numbers: \"7\"}\n",
        "containers:\n",
        "- name: api\n",
        "    image: inseefrlab/formation-mlops-api:main\n",
        "    imagePullPolicy: Always\n",
        "    env:\n",
        "    - name: MLFLOW_TRACKING_URI\n",
        "        value: https://projet-formation-******.user.lab.sspcloud.fr\n",
        "    - name: MLFLOW_MODEL_NAME\n",
        "        value: fasttext-model\n",
        "    - name: MLFLOW_MODEL_VERSION\n",
        "        value: \"1\"\n",
        "```\n",
        "\n",
        "4. On the [SSP Cloud](https://datalab.sspcloud.fr/home), launch an Argo-cd service by clicking [this URL](https://datalab.sspcloud.fr/launcher/automation/argo-cd?autoLaunch=true)\n",
        "5. Create a new application and use the yaml template stored in the `argocd` folder\n",
        "    a. `Create an application  -> Edit as YAML` \n",
        "    b. Adjust the yaml to your purpose\n",
        "    c. `Save -> Create`\n",
        "6. Reach your API using the URL you defined in your `ingress.yml` file\n",
        "7. Display the documentation of your API by adding `/docs` to your URL\n",
        "8. Try your API out!\n",
        "9. Re-run a new model and deploy this new model in your API\n",
        "\n",
        "<details>\n",
        "<summary>\n",
        "    <font size=\\\"3\\\" color=\\\"darkgreen\\\"><b>Click to see the steps </b></font>\n",
        "</summary>\n",
        "\n",
        "    + 1. Run a model\n",
        "    + 2. Register the model\n",
        "    + 3. Adjust your MLFLOW_MODEL_NAME or MLFLOW_MODEL_VERSION environment variable in the `deployment.yml` file\n",
        "    + 4. Commit and push these changes\n",
        "    + 5. Synchronise in Argo-cd or wait for 5 minutes\n",
        "    + 6. Refresh your API, it should be based on your new version!\n",
        "\n",
        "</details>\n",
        "\n",
        "::::\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Distributing the hyperparameter optimization\n",
        "\n",
        "## Parallel training\n",
        "\n",
        "- With our setup, we can train models [**one by one**]{.orange} and log all relevant information to the MLflow tracking server\n",
        "- What if we would like to train [**multiple models at once**]{.orange}, for example to optimize hyperparameters ?\n",
        "\n",
        "## Workflow automation\n",
        "\n",
        "- [**General principles**]{.orange} :\n",
        "    - Define workflows where each step in the workflow is a [**container**]{.blue2} (reproducibility)\n",
        "    - Model multi-step workflows as a [**sequence**]{.blue2} of tasks or as a [**directed acyclic graph**]{.blue2}\n",
        "    - This allows to easily [**run in parallel compute intensive jobs**]{.blue2} for machine learning or data processing\n",
        "\n",
        "## Argo workflows\n",
        "\n",
        "- A popular [**workflow engine**]{.orange} for orchestrating parallel jobs on `Kubernetes`\n",
        "  - [**open-source**]{.blue2}\n",
        "  - [**container-native**]{.blue2}\n",
        "  - available on the [**SSP Cloud**]{.orange}\n",
        "\n",
        ". . .\n",
        "\n",
        "![](img/argo-logo.png){fig-align=\"center\" height=300}\n",
        "\n",
        "## Hello World\n",
        "\n",
        "```yaml\n",
        "apiVersion: argoproj.io/v1alpha1\n",
        "kind: Workflow                  # new type of k8s spec\n",
        "metadata:\n",
        "  generateName: hello-world-    # name of the workflow spec\n",
        "spec:\n",
        "  entrypoint: whalesay          # invoke the whalesay template\n",
        "  templates:\n",
        "    - name: whalesay            # name of the template\n",
        "      container:\n",
        "        image: docker/whalesay\n",
        "        command: [ cowsay ]\n",
        "        args: [ \"hello world\" ]\n",
        "```\n",
        "\n",
        "## What is going on ?\n",
        "\n",
        ". . .\n",
        "\n",
        "![](img/argo-0.png){fig-align=\"center\" height=500}\n",
        "\n",
        "## What is going on ?\n",
        "\n",
        "![](img/argo-1a.png){fig-align=\"center\" height=500}\n",
        "\n",
        "## What is going on ?\n",
        "\n",
        "![](img/argo-2a.png){fig-align=\"center\" height=500}\n",
        "\n",
        "## Parameters\n",
        "\n",
        "- Templates can take [**input parameters**]{.orange}\n",
        "\n",
        ". . .\n",
        "\n",
        "```yaml\n",
        "apiVersion: argoproj.io/v1alpha1\n",
        "kind: Workflow\n",
        "metadata:\n",
        "  generateName: hello-world-parameters-\n",
        "spec:\n",
        "  entrypoint: whalesay\n",
        "  arguments:\n",
        "    parameters:\n",
        "    - name: message\n",
        "      value: hello world\n",
        "\n",
        "  templates:\n",
        "  - name: whalesay\n",
        "    inputs:\n",
        "      parameters:\n",
        "      - name: message       # parameter declaration\n",
        "    container:\n",
        "      image: docker/whalesay\n",
        "      command: [cowsay]\n",
        "      args: [\"{{inputs.parameters.message}}\"]\n",
        "```\n",
        "\n",
        "## Multi-step workflows\n",
        "\n",
        "- [**Multi-steps workflows**]{.orange} can be specified (`steps` or `dag`)\n",
        "\n",
        ". . .\n",
        "\n",
        "```yaml\n",
        "apiVersion: argoproj.io/v1alpha1\n",
        "kind: Workflow\n",
        "metadata:\n",
        "  generateName: steps-\n",
        "spec:\n",
        "  entrypoint: hello-hello-hello\n",
        "\n",
        "  # This spec contains two templates: hello-hello-hello and whalesay\n",
        "  templates:\n",
        "  - name: hello-hello-hello\n",
        "    # Instead of just running a container\n",
        "    # This template has a sequence of steps\n",
        "    steps:\n",
        "    - - name: hello1            # hello1 is run before the following steps\n",
        "        template: whalesay\n",
        "    - - name: hello2a           # double dash => run after previous step\n",
        "        template: whalesay\n",
        "      - name: hello2b           # single dash => run in parallel with previous step\n",
        "        template: whalesay\n",
        "  - name: whalesay              # name of the template\n",
        "    container:\n",
        "      image: docker/whalesay\n",
        "      command: [ cowsay ]\n",
        "      args: [ \"hello world\" ]\n",
        "```\n",
        "\n",
        "## What is going on ?\n",
        "\n",
        ". . .\n",
        "\n",
        "![](img/argo-0.png){fig-align=\"center\" height=500}\n",
        "\n",
        "## What is going on ?\n",
        "\n",
        "![](img/argo-1b.png){fig-align=\"center\" height=500}\n",
        "\n",
        "## What is going on ?\n",
        "\n",
        "![](img/argo-2b.png){fig-align=\"center\" height=500}\n",
        "\n",
        "## What is going on ?\n",
        "\n",
        "![](img/argo-1b.png){fig-align=\"center\" height=500}\n",
        "\n",
        "## What is going on ?\n",
        "\n",
        "![](img/argo-3b.png){fig-align=\"center\" height=500}\n",
        "\n",
        "## Further applications\n",
        "\n",
        "- Workflow [**to test**]{.orange} registered models, or models pushed to staging / production\n",
        "- Workflows can be [**triggered**]{.orange} automatically (via Argo Events for example)\n",
        "- [**Continuous training workflows**]{.orange}\n",
        "- [**Distributed**]{.orange} machine learning pipelines in general (data downloading, processing, etc.)\n",
        "\n",
        "## Further applications\n",
        "\n",
        ". . .\n",
        "\n",
        "![](img/pokemon_workflow.png){fig-align=\"center\" height=450}\n",
        "\n",
        "## Notes\n",
        "\n",
        "- [**Python SDK**]{.orange} for Argo Workflows\n",
        "- Kubeflow pipelines\n",
        "- [**Couler**]{.orange} : unified interface for constructing and managing workflows on different workflow engines\n",
        "- Other Python-native orchestration tools : [**Apache Airflow**]{.orange}, [**Metaflow**]{.orange}, [**Prefect**]{.orange}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Application 4\n",
        "\n",
        ":::{.callout-tip collapse=\"true\" icon=false}\n",
        "## Distributing the hyperparameter optimization with an orchestrator\n",
        "\n",
        ":::{.incremental}\n",
        "1. Open an Argo Workflows service and submit the `Hello World` workflow. Visualize the logs on the Argo Workflows UI.\n",
        "2. Take a look at the `argo_workflows/workflow.yml` file. What do you expect will happen when we submit this workflow ?\n",
        "3. Submit the workflow. Once all jobs are completed, visualize the logs. Then open the MLflow UI to check what has been done.\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Conclusion\n"
      ],
      "id": "01831f1c"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}